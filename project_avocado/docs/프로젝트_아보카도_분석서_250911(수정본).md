🥑 프로젝트 분석서: 아보카도 (Avocado)-수정중_250908
---

**AI Voice Communication Development Tool**


---

## **1. 프로젝트 개요**

### 1.1 프로젝트명

**아보카도 (Avocado)**

### 1.2 목적

- 발화 장애를 겪는 사용자에게 AI 기반 TTS(Text-to-Speech) 기능과
커스텀 음성 모델 생성 서비스를 제공하여
자연스럽고 인간다운 소통을 지원하는 것을 목적으로 한다.

### 1.3 주요 대상

- 언어 표현에 어려움을 겪는 발화 장애인
- 자신의 목소리를 AI로 구현하고 싶은 사용자

---

## **2. 전체 시스템 구조 (요약)**

> 본 프로젝트는 다음의 2가지 TTS 모델을 개발하여 서비스화합니다.
> 

### 2.1 기본 TTS 모델

- 성우 음성 기반, 사전 정의된 3종 화자 모델 제공
(예: 여성 청년, 남성 청년, 노년 남성)

### 2.2 커스텀 TTS 모델

- 사용자가 직접 음성을 업로드하여,
**자신의 목소리를 모사하는 AI 음성 모델 생성**

---

## **3. TTS 기술 개요**

개발 흐름에 대한 이해를 위해 기본적인 TTS 모델 제작 과정에 대한 설명을 첨부합니다.

### 3.1 작동 원리

`텍스트 입력 → 언어 처리 → 음향 모델 → 보코더 → 음성 출력`

### 3.2 TTS 개발 프로세스

### 1) Text Analysis (언어 처리)

- 입력된 텍스트 데이터에서 의미있는 정보와 패턴을 자동으로 찾는 과정
- 한국어 특성 고려 (띄어쓰기 오류, 동음이의어 처리 등)

### 2) Acoustic Model (음향 모델)

- 소리 → 글자
- 음향 신호를 음소(가장 작은 단위의 발음 기호)로 변환

### 3) Vocoder (보코더)

- 글자 → 소리
- 음소 정보를 음성 파형으로 변환

---

# **[Part 1] 기본 TTS 모델**

---

## **1. 기반 엔진: Coqui XTTS 사전 학습 모델 채택**

### 채택 이유

- 고품질 사전 학습 모델 제공
- 자연스러운 음질 제공
- Few-shot 커스터마이징 가능

> 처음부터 모델을 학습하지 않고 Coqui XTTS의 사전 학습 모델을 활용하여
효율적이고 품질 높은 시스템을 구축한다.
> 

---

## **2. 구현 프로세스**

`[사용자 음성 수집] → [데이터 전처리] → [화자 임베딩 추출] → [텍스트 입력 → 음성 합성] → [음성 출력 + 저장 + 평가]`

### 2.1 음성 데이터 수집 및 전처리

### 수집 기준

- AI Hub 공개 데이터셋 사용 (전문 성우 음성 기반)
- 최소 3시간 이상의 음성 확보
- **파일 조건**:
    - 형식: `.wav`
    - 샘플링: `22050Hz` 또는 `44100Hz`
    - 비트: `16-bit`, 채널: `Mono`

### 전처리 단계

1. **묵음 제거 및 정규화**
    - `librosa`, `pydub` 등을 이용해 음성에서 긴 정적 구간 제거
    - 전체 음성 볼륨을 RMS 기반으로 정규화
2. **텍스트 정규화**
    - 숫자 → 발음 표기로 치환 ("123" → "백이십삼")
    - 약어 및 기호 변환 ("Dr." → "박사")
    - 괄호, 슬래시, 하이픈 등 비발음 기호 제거

### 디렉터리 구조 예시

```
/dataset/
├── voice_A/
│   ├── wavs/
│   │   ├── voice_A_0001.wav
│   └── metadata.csv  # [파일명 | 텍스트 | 음소]

```

---

### 2.2 화자 임베딩 추출 (Speaker Embedding)

- Coqui XTTS는 사전 학습된 스피커 인코더 내장
- 짧은 음성 입력으로 화자 임베딩 벡터 생성

```python
speaker_embedding = xtts_model.get_speaker_embedding("sample.wav")

```

- 임베딩은 음색, 톤, 속도 등만 반영
- 발음과 억양은 기본 모델이 담당

---

### 2.3 텍스트 → 음성 합성 (Inference)

### 처리 흐름

1. 유저 텍스트 입력 수신
2. 텍스트 + 화자 임베딩을 XTTS에 입력
3. Mel-spectrogram 생성
4. 내장 보코더(HiFi-GAN)로 음성 파형 복원

```python
output_audio = xtts_model.synthesize(
    text="안녕하세요",
    speaker_embedding=speaker_embedding
)

```

### 적용 기술

- Text Encoder: BERT 기반 문장 임베딩
- Acoustic Decoder: FastSpeech2 / Transformer
- Vocoder: HiFi-GAN

---

### 2.4 음성 출력, 저장, 응답

- 출력 포맷: `.wav`, `.mp3`
- 예시 응답:

```json
{
  "filename": "output_20250908_001.wav",
  "url": "<https://s3.amazonaws.com/avocado/output_20250908_001.wav>",
  "duration": "3.2s"
}

```

---

### 2.5 음성 품질 평가

### 주관적 평가 (MOS)

- 항목: 자연스러움, 명료성, 감정 표현
- 1~5점 척도로 평가

### 객관적 평가

- Mel-spectrogram 유사도
- 정량 지표 예시: PESQ, cosine similarity

---

## **3. 구현 목표**

- **명확성(Intelligibility):** 정확하고 이해 가능한 발음
- **자연스러움(Naturalness):** 감정과 억양이 반영된 사람다운 발화

---

# **[Part 2] 커스텀 TTS 모델**

---

## **1. 기반 엔진: Coqui XTTS 다중화자 모델 채택**

- 본 프로젝트는 스피커 인코더를 직접 설계하지 않고, Coqui XTTS에 내장된 사전 학습 스피커 인코더를 그대로 활용

### 채택 이유

- 위와 동일 (고품질 음성, 개발 리소스 절약, Few-shot 학습 지원)

---

## **2. 커스텀 음성 모델 생성 방식**

| 항목 | 스피커 임베딩 방식 | 엔드투엔드 파인튜닝 |
| --- | --- | --- |
| 핵심 원리 | '목소리 특징'과 '발음'을 분리 | '목소리'와 '발음'을 함께 학습 |
| 결과물 | 깨끗한 발음 + 사용자의 고유한 목소리 톤 | 어눌한 발음 + 사용자의 목소리 톤 |
| 장점 | 목표에 완벽히 부합, 발음 오류 학습 방지 | 미세한 뉘앙스까지 학습할 잠재력 |
| 단점 | 음성 지문 추출 모델 성능에 의존 | 발음 오류까지 그대로 학습할 위험 매우 큼 |
| 비유 | 전문 성우가 목소리 톤만 흉내 내는 것 | 배우가 어눌한 말투까지 똑같이 따라 하는 것 |
- 엔드투엔드 파인튜닝 시 불완전한 발음이 그대로 학습될 위험 높음
- 불완전한 발음은 학습 대상에서 제외
- 발음과 내용과 관계없이 목소리 고유 특징(음색, 높낮이, 억양 스타일 등)만을 추출하여 '스피커 임베딩'이라는 지문을 생성

**⇒ Coqui XTTS 모델에 스피커 임베딩 방식으로 학습**

---

## **3. 시스템 아키텍처**

```
[사용자 음성 입력]
→ [음성 전처리]
→ [스피커 인코더(XTTS)]
→ [스피커 임베딩 추출]
→ [DB 저장]
→ [TTS 모델 입력]
→ [음성 합성]

```

---

## **4. 구성 요소 및 개발 단계**

### 4.1 사용자 음성 샘플 수집 및 전처리

- 입력 형식: `.wav`, `.mp3`
- 수집 가이드: 조용한 환경, 3분 이상 녹음
- 전처리:
    - 음성 구간 분리 (VAD): 업로드된 파일에서 불필요한 앞뒤 묵음 구간을 제거하여 목소리 부분만 추출.
    - 음성 정규화: 전체 음성 볼륨을 RMS 기준으로 일정하게 정규화하여 모델 입력 품질을 높임.
    - 입력 검증: 너무 짧거나 품질이 낮은 음성 파일은 사용자에게 피드백하여 재업로드를 유도.

### 4.2 임베딩 추출 및 저장

- `get_speaker_embedding()` 함수로 임베딩 생성
- MySQL DB에 사용자 ID와 함께 저장

### 4.3 TTS 모델 통합

- 입력: 텍스트 + 임베딩
- 결과: 해당 사용자의 톤으로 합성된 음성

---

## **5. 서비스 품질 평가 및 검증**

### 5.1 주관적 평가 (MOS)

- 항목: 자연스러움, 명료성, 화자 유사성
- 1~5점 척도

### 5.2 객관적 검증

- 다양한 입력 품질 → 일관된 결과 여부
- 임베딩 실패 시 사용자 피드백 제공 여부

---

## **6. 기술 스택 요약**

| 분야 | 기술 |
| --- | --- |
| 프론트엔드 | React, HTML/CSS |
| 백엔드 | Node.js (Express) |
| AI 모델 | Python, PyTorch, Coqui XTTS |
| 데이터베이스 | MySQL |
| 클라우드 | 미정 |

---

## **7. 데이터 모델 및 저장소 구성**

### 7.1 주요 데이터 모델

- **User**: 사용자 정보 (이름, 이메일, 성별, 나이 등)
- **VoiceModel**: 기본/커스텀 음성 모델 정보
- **OutputVoice**: 생성된 음성 결과

### 7.2 저장 방식

- DB(MySQL): 구조화된 정보 저장
- 클라우드: 음성 파일 저장, DB에는 파일 경로만 저장

---

## **8. 사용 시나리오**

### 8.1 텍스트 → 음성 변환

1. 로그인
2. 텍스트 입력
3. 모델 선택(기본 또는 커스텀 모델)
4. 변환 요청→ 음성 생성
5. 결과 재생 / 다운로드 / 이메일 발송

### 8.2 커스텀 음성 모델 생성

1. “내 목소리 모델” 메뉴 진입
2. 지정 문장 녹음
3. 업로드 및 커스텀 모델 생성
4. 이후 모텔 선택 가능

### 8.3 음성 관리

- 생성된 음성 내역 조회
- 재생 / 다운로드 / 삭제 기능 제공

---

## **9. UI/UX 가이드라인**

- **간결성**: 직관적이고 단순한 사용자 인터페이스

---