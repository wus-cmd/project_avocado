네, 우스님! 요청하신 대로 내용 수정 없이 `[cite]` 링크만 제거해 드릴게요.

***

### **프로젝트 분석서: 아보카도 (Avocado)**
**AI Voice Communication Development Tool**

---

### **1. 프로젝트 개요**

**1.1 프로젝트명**
아보카도 (Avocado)

**1.2 목적**
발화 장애를 겪는 사용자에게 AI 기반 TTS(Text-to-Speech) 기능과 커스텀 음성 모델 생성 서비스를 제공하여 자연스럽고 인간다운 소통을 지원하는 것을 목적으로 한다.

**1.3 주요 대상**
* 언어 표현에 어려움을 겪는 발화 장애인
* 자신의 목소리를 AI로 구현하고 싶은 사용자

---

### **2. 전체 시스템 구조 (요약)**
본 프로젝트는 다음의 2가지 TTS 모델을 개발하여 서비스화합니다.

**2.1 기본 TTS 모델**
사전 정의된 남성, 여성, 캐릭터 3종의 화자 모델을 제공한다.

**2.2 커스텀 TTS 모델**
사용자가 직접 음성을 업로드하여, 자신의 목소리를 모사하는 AI 음성 모델을 생성한다.

---

### **[Part 1] 기본 TTS 모델**

**1. 기반 엔진: Coqui XTTS v2 사전 학습 모델 채택**

* **채택 이유**
    * **고품질 사전 학습 모델 제공:** 처음부터 대규모 데이터로 모델을 학습시킬 필요 없이, 이미 자연스러운 음질을 생성하도록 학습된 모델을 활용할 수 있어 개발 효율이 높다.
    * **자연스러운 음질:** 내장된 고성능 보코더(HiFi-GAN)를 통해 기계음 같지 않고 사람처럼 자연스러운 음성을 합성한다.
    * **Zero-Shot / Few-shot 커스터마이징:** 단 몇 초의 음성 샘플만으로도 새로운 화자의 목소리를 복제(Zero-Shot)할 수 있는 강력한 기능을 제공하여, 커스텀 모델 구현에 매우 유리하다.
    * 처음부터 모델을 학습하지 않고 Coqui XTTS의 사전 학습 모델을 활용하여 효율적이고 품질 높은 시스템을 구축한다.

**2. 구현 프로세스**
[AI 서버 구동] → [텍스트 및 모델 정보 수신] → [화자 임베딩 활용] → [음성 합성] → [결과 응답]

* **2.1 AI 서버 및 모델 로딩**
    * **FastAPI 기반 서버 구축:** AI 모델 파트는 FastAPI를 기반으로 RESTful API 서버로 구축되었으며, 백엔드 서버와 HTTP 통신을 통해 연동된다.
    * **사전 모델 로딩:** AI 서버가 시작될 때 Coqui XTTS v2 모델을 미리 메모리에 로드하여(tts_model = TTS("...")), 실제 변환 요청 시 발생하는 지연을 최소화한다.

* **2.2 텍스트 → 음성 합성 (Inference)**
    * **처리 흐름**
        * 백엔드 서버로부터 POST /synthesize 엔드포인트로 text, speaker\_wav, user\_id를 포함한 요청을 수신한다.
        * 요청받은 speaker\_wav 값(예: conan.wav, jjanggu.wav 등)을 기반으로 사전에 정의된 참조 음성 파일의 경로를 가져온다.
        * tts\_model.tts\_to\_file() 함수에 텍스트와 참조 음성 경로를 입력하여 음성 합성을 수행한다. 이때 XTTS 모델이 내부적으로 참조 음성에서 스피커 임베딩을 추출하여 활용한다.
        * 합성된 음성 파일은 outputs/ 디렉토리에 user\_{user\_id}\_{base\_model\_name}\_{timestamp}.wav 형식으로 저장된다.
        * 생성된 파일명을 백엔드 서버에 JSON 형태로 응답한다.

---

### **[Part 2] 커스텀 TTS 모델**

**1. 커스텀 음성 모델 생성 방식**

| 항목 | 스피커 임베딩 방식 (채택) | 엔드투엔드 파인튜닝 (미채택) |
| :--- | :--- | :--- |
| **핵심 원리** | '목소리 특징'과 '발음'을 분리 | '목소리'와 '발음'을 함께 학습 |
| **결과물** | 깨끗한 발음 + 사용자의 고유한 목소리 톤 | 어눌한 발음 + 사용자의 목소리 톤 |
| **장점** | 프로젝트 목표에 완벽히 부합, 발음 오류 학습 방지 | 미세한 뉘앙스까지 학습할 잠재력 |
| **단점** | 음성 지문 추출 모델 성능에 의존 | 발음 오류까지 그대로 학습할 위험 매우 큼 |
| **비유** | 전문 성우가 목소리 톤만 흉내 내는 것 | 배우가 어눌한 말투까지 똑같이 따라 하는 것 |

* 발화가 어려운 사용자의 불완전한 발음이 모델에 그대로 학습되는 것을 방지하는 것이 핵심 과제였다.
* 스피커 임베딩 방식은 발음과 관계없이 목소리의 고유한 특징(음색, 높낮이, 억양 스타일 등)만을 추출하여 '음성 지문'을 생성한다.
* 이 방식은 원본 XTTS 모델의 깨끗하고 정확한 발음은 그대로 유지하면서, 목소리 톤만 사용자의 것으로 바꾸기 때문에 프로젝트의 목적에 가장 부합한다.
    ⇒ Coqui XTTS v2 모델의 Zero-Shot 스피커 임베딩 방식을 채택하여 커스텀 모델 기능을 구현한다.

**2. 시스템 아키텍처**
[사용자 음성 업로드 (프론트엔드)] → [파일 저장 및 경로 전달 (백엔드)] → [음성 전처리 및 임베딩 추출 (AI 서버)] → [DB 저장 (백엔드)] → [TTS 모델 입력] → [음성 합성]

**3. 구성 요소 및 개발 단계**

* **3.1 사용자 음성 샘플 수집 및 전처리**
    * **입력 형식:** .wav, .mp3
    * **수집 가이드:** 사용자는 조용한 환경에서 1~3분 이상 녹음한 자유로운 음성 파일을 업로드한다.
    * **전처리 (AI 서버):**
        * **음성 구간 분리 (VAD):** 업로드된 파일에서 불필요한 앞뒤 묵음 구간을 제거하여 목소리 부분만 추출한다.
        * **음성 정규화:** 전체 음성 볼륨을 RMS 기준으로 일정하게 정규화하여 모델 입력 품질을 높인다.
        * **입력 검증:** 너무 짧거나 품질이 낮은 음성 파일은 사용자에게 피드백하여 재업로드를 유도한다.

* **3.2 임베딩 추출 및 활용**
    * **추출:** AI 서버는 전처리된 음성 파일 경로를 tts\_to\_file 함수의 speaker\_wav 인자로 받아 실시간으로 스피커 임베딩을 추출하고 이를 즉시 음성 합성에 사용한다 (Zero-Shot 방식).
    * **저장:** 생성된 커스텀 음성 모델(참조 음성 파일) 정보는 백엔드 DB의 VoiceModel 테이블에 사용자 ID와 함께 저장된다.

**4. 기술 스택 및 데이터 모델**

* **4.1 기술 스택 요약**

| 분야 | 기술 |
| :--- | :--- |
| **프론트엔드** | React, Vite, Tailwind CSS |
| **백엔드** | Node.js (Express), JWT, bcrypt, multer |
| **AI 모델**| Python, FastAPI, PyTorch, Coqui XTTS v2 |
| **데이터베이스** | MySQL |
| **클라우드** | 미정 |

* **4.2 데이터 모델**
    * **User:** 사용자 정보 (id, email, pw, name, age, gender, createdAt)
    * **VoiceModel:** 음성 모델 정보 (id, userId, type, name, embedding\_vector)
    * **OutputVoice:** 생성된 음성 결과 (id, userId, modelId, text, fileUrl, createdAt)

**5. 사용 시나리오**

* **5.1 회원가입 및 로그인**
    * 사용자는 이메일, 비밀번호, 이름, 생년월일, 성별을 입력하여 회원가입한다.
    * 가입된 정보로 로그인하며, 비밀번호 분실 시 이메일 인증을 통해 재설정할 수 있다.

* **5.2 텍스트 → 음성 변환**
    * 로그인 후, 텍스트를 입력하고 기본 모델(남성, 여성, 캐릭터) 또는 저장된 커스텀 모델을 선택한다.
    * '변환' 요청 시 시스템은 음성 파일을 생성한다.
    * 사용자는 결과를 커스텀 플레이어로 즉시 재생하거나 MP3, WAV 파일로 다운로드할 수 있다.

* **5.3 커스텀 음성 모델 생성**
    * ‘내 목소리 만들기’ 메뉴로 진입한다.
    * 시스템의 가이드에 따라 1~3분 가량의 음성 파일을 업로드한다.
    * 시스템은 업로드된 음성에서 사용자의 고유한 음색 특징(스피커 임베딩)을 추출하여 커스텀 모델을 생성한다.
    * 생성 완료 후, 음성 변환 시 '내 목소리 모델'을 선택하여 활용할 수 있다.

* **5.4 변환 기록 관리**
    * 마이페이지에서 자신의 모든 음성 변환 기록을 최신순으로 조회할 수 있다.
    * 각 기록은 다시 듣거나 삭제할 수 있다.