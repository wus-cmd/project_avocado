## 1\. 개요

### 1.1. AI 모델링의 역할과 범위

본 프로젝트의 AI 서버는 핵심 기능인 **TTS(Text-to-Speech) 음성 합성** 및 **커스텀 음성 모델 생성에 필요한 스피커 임베딩 활용**을 담당합니다. 백엔드 서버의 요청을 받아 입력 텍스트와 스피커 정보를 기반으로 새로운 음성 파일을 생성한 후, 파일명과 처리 결과를 응답합니다.

### 1.2. 전체 시스템 아키텍처 내 AI 파트 위치

AI Model 파트는 **FastAPI**를 기반으로 **RESTful API 서버**로 구축되었으며, 백엔드 서버와 **HTTP 통신**을 통해 연동됩니다. AI 서버는 포트 **8000**에서 실행되며, 백엔드 서버는 이 엔드포인트(`http://127.0.0.1:8000/synthesize`)로 요청을 보냅니다.

-----

## 2\. AI 모델 상세 설계 및 개발 환경

### 2.1. 기술 스택 및 라이브러리

| 구분 | 기술 | 세부 내용 | 출처 |
| :--- | :--- | :--- | :--- |
| **프레임워크** | **FastAPI** | 백엔드와의 통신을 위한 API 서버 프레임워크 | |
| **웹 서버** | **Uvicorn** | FastAPI를 구동하는 ASGI 서버 | |
| **핵심 모델** | **TTS (Coqui XTTS v2)** | 다국어(Multilingual) 지원 및 Zero-Shot 음성 합성 능력을 가진 모델 | |
| **딥러닝** | **PyTorch** | 딥러닝 모델 구동의 기반 라이브러리 | |
| **유틸리티** | **Transformers** | 딥러닝 모델 관련 유틸리티 및 컴포넌트 | |

  * **환경 설정**: Python **3.11** 버전으로 가상 환경(`venv`)을 생성하고 의존성을 설치합니다.
  * **추가 인덱스 URL**: CPU 환경을 위한 PyTorch 패키지를 설치하기 위해 `--extra-index-url https://download.pytorch.org/whl/cpu`가 `requirements.txt`에 명시되어 있습니다.

### 2.2. 모델 아키텍처 선정 및 활용

  * **선정 모델**: **`tts_models/multilingual/multi-dataset/xtts_v2`** 모델이 사용됩니다.
  * **사전 로딩**: 서버 시작 시, **`main.py`** 파일에서 해당 XTTS v2 모델을 메모리에 로드하여(`tts_model = TTS("...")`) 요청 시 지연을 최소화합니다. 모델은 `download_model.py`를 통해 사전에 다운로드 및 캐싱됩니다.

-----

## 3\. 핵심 모델 구현 내용

### 3.1. 텍스트-음성 변환 (TTS) 모델 구현

  * **API 엔드포인트**: `POST /synthesize`.

  * **요청 데이터 모델 (`TTSRequest`)**:

    ```python
    class TTSRequest(BaseModel):
        text: str       # 변환할 텍스트
        speaker_wav: str  # 참조 음성 파일명 (e.g., 'sample.wav')
        user_id: int    # 사용자 식별자
    ```

  * **합성 로직**:

    1.  요청에서 받은 `speaker_wav`를 사용하여 서버의 `voices/` 디렉토리 내의 **스피커 참조 파일 경로**(`speaker_wav_path`)를 구성합니다.
    2.  참조 파일이 존재하지 않으면 에러를 반환합니다.
    3.  `tts_model.tts_to_file()` 함수를 사용하여 음성 합성을 수행합니다.
    4.  **언어 설정**은 \*\*`language="ko"`\*\*로 명시되어 한국어 합성을 진행합니다.

  * **출력 파일 관리**:

    1.  출력 파일은 **`outputs`** 디렉토리에 저장되며, 해당 디렉토리가 없으면 생성됩니다.
    2.  파일명은 **`user_{user_id}_{base_model_name}_{timestamp}.wav`** 형식으로, 사용자 ID와 타임스탬프를 포함하여 고유성을 확보합니다.

### 3.2. 커스텀 음성 파일 처리 및 스피커 임베딩 활용

  * **커스텀 파일 위치**: 백엔드에서 업로드된 사용자 음성 샘플 파일은 AI 서버의 **`voices/`** 디렉토리에 위치한다고 가정하고 경로가 구성됩니다.
  * **Zero-Shot 임베딩**: XTTS v2 모델은 `tts_to_file` 함수에 `speaker_wav` 경로를 직접 입력받아, 해당 파일에서 **스피커 임베딩**을 추출하고 이를 기반으로 텍스트를 합성하는 Zero-Shot TTS 방식으로 커스텀 모델 기능을 구현합니다.

-----

## 4\. 백엔드 시스템 연동 방안

### 4.1. API 명세 및 통신 프로토콜

  * **호출 경로**: `POST http://127.0.0.1:8000/synthesize`.
  * **응답 형식**: 합성 성공 시, 생성된 음성 파일의 이름(`filename`)과 성공 메시지를 JSON 형태로 반환합니다.

### 4.2. 시스템 실행 명령어

  * **실행 명령**: AI 서버는 가상 환경을 활성화한 후, **`uvicorn main:app --port 8000`** 명령어를 통해 실행됩니다.

-----

## 5\. 결론 및 향후 개선 과제

### 5.1. 개발 완료 내용 요약 및 성능 평가 결과

  * **안정적인 API 구축**: FastAPI 기반으로 **`synthesize`** 엔드포인트를 구현하여 백엔드의 요청을 처리할 수 있는 구조를 완성했습니다.
  * **확보된 기능**: Zero-Shot TTS 기술을 통해 사용자 맞춤형 음색으로 한국어 음성 합성이 가능합니다.

### 5.2. 향후 개선 과제

  * **성능 최적화**: 현재 `requirements.txt`에 명시된 PyTorch 설치 옵션(`--extra-index-url https://download.pytorch.org/whl/cpu`)은 **CPU 환경**에 맞춰져 있어, 실제 서비스 운영 시 **GPU 환경**으로 변경하여 추론 속도를 획기적으로 개선할 필요가 있습니다.
  * **오류 처리**: 현재 `speaker_wav` 파일이 없을 경우에 대한 오류 처리 로직은 구현되어 있지만, TTS 모델 내부에서 발생하는 예외(e.g., 메모리 부족, 모델 로드 실패 등)에 대한 **`try-except` 블록**을 추가하여 안정성을 높일 수 있습니다.
  * **파일 제공 연동**: AI 서버가 생성한 음성 파일은 `outputs` 폴더에 저장되는데, 이 파일들을 프론트엔드에서 접근할 수 있도록 백엔드 서버의 정적 파일 제공 경로와 연동하는 작업이 필수입니다.